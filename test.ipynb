{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/self_rag/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "access_token = \"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\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "     token=access_token\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_labels = ['NAME_STUDENT', 'EMAIL', 'USERNAME', 'ID_NUM', 'PHONE_NUM', 'URL_PERSONAL', 'STREET_ADDRESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str):\n",
    "    return f'''<s>\n",
    "[INST]\n",
    "<<SYS>>\n",
    "You are a helpful and honest assistant. You will list the instances of words that match one of the following categories: \n",
    "<</SYS>>\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "My name is Bryce and my sister's name is Sara. My email is tombombadill@gmail.com and my contact number is 830 688 0393.\n",
    "[/INST]\n",
    "OUTPUT:\n",
    "Bryce (NAME_STUDENT),\n",
    "Sara (NAME_STUDENT),\n",
    "tombombadill@gmail.com (EMAIL),\n",
    "830 688 0393 (PHONE_NUM)\n",
    "</s>\n",
    "<s>\n",
    "[INST]\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "John Doe, I live in the 123 Main Street. My website is www.seanhalpin.xyz and my contact number is 888-688-5461.\n",
    "[/INST]\n",
    "OUTPUT:\n",
    "John Doe (NAME_STUDENT),\n",
    "123 Main Street (STREET_ADDRESS),\n",
    "www.seanhalpin.xyz (URL_PERSONAL)\n",
    "830-688-0393 (PHONE_NUM)\n",
    "</s>\n",
    "<s>\n",
    "[INST]\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "{prompt}\n",
    "[/INST]\n",
    "OUTPUT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str):\n",
    "    return f'''You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student’s email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "My name is Bryce and my sister's name is Sara. My email is tombombadill@gmail.com and my contact number is 830 688 0393.\n",
    "OUTPUT:\n",
    "Bryce (NAME_STUDENT),\n",
    "Sara (NAME_STUDENT),\n",
    "tombombadill@gmail.com (EMAIL),\n",
    "830 688 0393 (PHONE_NUM)\n",
    "\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "John Doe, I live in the 123 Main Street. My website is www.seanhalpin.xyz and my contact number is 888-688-5461.\n",
    "OUTPUT:\n",
    "John Doe (NAME_STUDENT),\n",
    "123 Main Street (STREET_ADDRESS),\n",
    "www.seanhalpin.xyz (URL_PERSONAL)\n",
    "830-688-0393 (PHONE_NUM)\n",
    "\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "{prompt}\n",
    "OUTPUT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Diego Estrada Design Thinking Assignment Visualization Tool Challenge & Selection The elderly were having a hard time adapting to the changes we brought in our bank. As a result of a poorly implemented linear solution, a more customer centric approach was needed. After learning about design thinking in this course, Javier decided to apply it to solve this problem, then called 210-999-0999 at 22211 Escalante Run address. The visualization tool allowed the team to create a dynamic presentation using diagrams, figures and drawings on the go that really resonated among the stakeholders. Previous to this change, none of our solutions seemed to be adequate for them, but the new implementation created a different type of connection with them that helped them understand the problem in the way the team and I did https://github.com/jdelarosaquiros\"\n",
    "prompt = format_prompt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diego Estrada (NAME_STUDENT),\n",
      "Javier (NAME_STUDENT),\n",
      "210-999-0999 (PHONE_NUM),\n",
      "22211 Escalante Run (STREET_ADDRESS),\n",
      "https://github.com/jdelarosaquiros (URL_PERSONAL)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=1500,\n",
    "    temperature=0.01,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\".replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nDiego Estrada (NAME_STUDENT)',\n",
       " '\\nJavier (NAME_STUDENT)',\n",
       " '\\n210-999-0999 (PHONE_NUM)',\n",
       " '\\n22211 Escalante Run (STREET_ADDRESS)',\n",
       " '\\nhttps://github.com/jdelarosaquiros (URL_PERSONAL)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print(len(prompt))\n",
    "outputs = re.split(r',', seq['generated_text'].replace(prompt, \"\"))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diego Estrada (NAME_STUDENT)',\n",
       " 'Javier (NAME_STUDENT)',\n",
       " '210-999-0999 (PHONE_NUM)',\n",
       " '22211 Escalante Run (STREET_ADDRESS)',\n",
       " 'https://github.com/jdelarosaquiros (URL_PERSONAL)']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pii_labels_pattern = '|'.join(pii_labels)\n",
    "outputs = [output.strip() for output in outputs if re.search(pii_labels_pattern, output)]\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diego Estrada (NAME_STUDENT)\\nJavier (NAME_STUDENT)\\n210-999-0999 (PHONE_NUM)\\n22211 Escalante Run (STREET_ADDRESS)\\nhttps://github.com/jdelarosaquiros (URL_PERSONAL)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = '\\n'.join(outputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def find_sequence_indices(list_words, sequence_to_find):\n",
    "    sequence_length = len(sequence_to_find)\n",
    "    indices = [i for i in range(len(list_words) - sequence_length + 1) if list_words[i:i+sequence_length] == sequence_to_find]\n",
    "    return indices\n",
    "\n",
    "def llama_to_tokens(output):\n",
    "    nlp = English()\n",
    "\n",
    "    english_tokenizer = nlp.tokenizer\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    answers = re.split(r'\\n',output)\n",
    "    print(\"Answers\", answers)\n",
    "    for i in range(len(answers)):\n",
    "        tokens.append(re.split(r'\\(|\\)', answers[i])[:-1])\n",
    "        print(re.split(r'\\(|\\)', answers[i])[:-1])\n",
    "\n",
    "        labels.append(tokens[-1][-1])\n",
    "        tokens[-1] = tokens[-1][:-1]\n",
    "\n",
    "    print('Tokens', tokens)\n",
    "    print('Labels', labels)\n",
    "    for i in range(len(tokens)):\n",
    "        print(tokens[i][0])\n",
    "        tokenized = english_tokenizer(tokens[i][0])\n",
    "        print('tokenized', tokenized)\n",
    "        tokens[i] = [i.text for i in tokenized]\n",
    "\n",
    "    \n",
    "    print(tokens,'\\n', labels)\n",
    "    return tokens, labels\n",
    "\n",
    "def categorizer(full_token_list, llm_tokens, labels):\n",
    "    indices = []\n",
    "    for i in range(len(llm_tokens)):\n",
    "        indices.append(find_sequence_indices(full_token_list, llm_tokens[i]))\n",
    "    print(indices)\n",
    "    result = ['O'] \n",
    "    len(full_token_list) # This will be a list of length full_tokens_list\n",
    "\n",
    "\n",
    "    for k in range(len(llm_tokens)):\n",
    "        for i in range(len(indices[k])):\n",
    "            result[indices[k][i]] = 'B-'+labels[k]\n",
    "            result[indices[k][i]+1:indices[k][i]+len(llm_tokens[k])] = 'I-'+labels[i]\n",
    "            if len(llm_tokens[k])>1:\n",
    "                for l in range(len(llm_tokens[k])-1):\n",
    "                    result[indices[k][i]+l+1] = 'I-' + labels[k]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "english_tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text: The strange thing said, \"Beep\". I called into the fog, \"What did you say?\" Out as a response was, \"Beep!\"\n",
      "LLM Output: Beep (HIVER)\n",
      "What did you say? (GREEN_LANDER)\n",
      "Answers ['Beep (HIVER)', 'What did you say? (GREEN_LANDER)']\n",
      "['Beep ', 'HIVER']\n",
      "['What did you say? ', 'GREEN_LANDER']\n",
      "Tokens [['Beep '], ['What did you say? ']]\n",
      "Labels ['HIVER', 'GREEN_LANDER']\n",
      "Beep \n",
      "tokenized Beep \n",
      "What did you say? \n",
      "tokenized What did you say? \n",
      "[['Beep'], ['What', 'did', 'you', 'say', '?']] \n",
      " ['HIVER', 'GREEN_LANDER']\n",
      "Text tokens: [['Beep'], ['What', 'did', 'you', 'say', '?']] Labels: ['HIVER', 'GREEN_LANDER']\n",
      "[[6, 29], [16]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m text, labels \u001b[38;5;241m=\u001b[39m llama_to_tokens(text)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText tokens:\u001b[39m\u001b[38;5;124m'\u001b[39m,text,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels:\u001b[39m\u001b[38;5;124m'\u001b[39m,labels)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Output:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mcategorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[32], line 49\u001b[0m, in \u001b[0;36mcategorizer\u001b[0;34m(full_token_list, llm_tokens, labels)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(llm_tokens)):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indices[k])):\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mlabels[k]\n\u001b[1;32m     50\u001b[0m         result[indices[k][i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:indices[k][i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(llm_tokens[k])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mlabels[i]\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(llm_tokens[k])\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "full_text = \"\"\"The strange thing said, \\\"Beep\\\". I called into the fog, \\\"What did you say?\\\" Out as a response was, \\\"Beep!\\\"\"\"\"\n",
    "print('full_text:',full_text)\n",
    "nlp = English()\n",
    "english_tokenizer = nlp.tokenizer\n",
    "tokenized = english_tokenizer(full_text)\n",
    "full_text = [i.text for i in tokenized]\n",
    "\n",
    "# print(full_text)\n",
    "text = \"\"\"Beep (HIVER)\n",
    "What did you say? (GREEN_LANDER)\"\"\"\n",
    "print('LLM Output:',text)\n",
    "\n",
    "text, labels = llama_to_tokens(text)\n",
    "print('Text tokens:',text,'Labels:',labels)\n",
    "\n",
    "\n",
    "print('Final Output:',categorizer(full_text,text, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diego Estrada (NAME_STUDENT)\\nVisualization Tool (URL_PERSONAL)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text: Diego Estrada Design Thinking Assignment Visualization Tool Challenge & Selection The elderly were having a hard time adapting to the changes we brought in our bank. As a result of a poorly implemented linear solution, a more customer centric approach was needed. After learning about design thinking in this course, we decided to apply it to solve this problem. The visualization tool allowed the team to create a dynamic presentation using diagrams, figures and drawings on the go that really resonated among the stakeholders. Previous to this change, none of our solutions seemed to be adequate for them, but the new implementation created a different type of connection with them that helped them understand the problem in the way the team and I did. Application The process starts in the prep time. The team uses a series of tools and software to develop a presentation using the surveys gathered during research and the solutions we created during the process.\n",
      "LLM Output: Diego Estrada (NAME_STUDENT)\n",
      "Visualization Tool (URL_PERSONAL)\n",
      "Text tokens: [[], []] Labels: ['', '']\n"
     ]
    }
   ],
   "source": [
    "full_text = text\n",
    "print('full_text:',text)\n",
    "tokenized = english_tokenizer(full_text)\n",
    "full_text = [i.text for i in tokenized]\n",
    "\n",
    "# print(full_text)\n",
    "\n",
    "print('LLM Output:',output)\n",
    "\n",
    "output, labels = llama_to_tokens(output)\n",
    "print('Text tokens:',output,'Labels:',labels)\n",
    "\n",
    "# print('Final Output:',categorizer(full_text,text, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
