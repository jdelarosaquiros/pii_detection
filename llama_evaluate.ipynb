{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "from spacy.lang.en import English\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/self_rag/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "     token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "english_tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_labels = ['NAME_STUDENT', 'EMAIL', 'USERNAME', 'ID_NUM', 'PHONE_NUM', 'URL_PERSONAL', 'STREET_ADDRESS']\n",
    "pii_labels_pattern = '|'.join(pii_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str):\n",
    "    return f'''You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student’s email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "My name is Bryce and my sister's name is Sara. My email is tombombadill@gmail.com and my contact number is 830 688 0393.\n",
    "OUTPUT:\n",
    "Bryce (NAME_STUDENT),\n",
    "Sara (NAME_STUDENT),\n",
    "tombombadill@gmail.com (EMAIL),\n",
    "830 688 0393 (PHONE_NUM)\n",
    "\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "John Doe, I live in the 123 Main Street. My website is www.seanhalpin.xyz and my contact number is 888-688-5461.\n",
    "OUTPUT:\n",
    "John Doe (NAME_STUDENT),\n",
    "123 Main Street (STREET_ADDRESS),\n",
    "www.seanhalpin.xyz (URL_PERSONAL)\n",
    "830-688-0393 (PHONE_NUM)\n",
    "\n",
    "You are searching for these different types of words:\n",
    "\n",
    "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
    "EMAIL - A student's email address.\n",
    "USERNAME - A student's username on any platform.\n",
    "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
    "PHONE_NUM - A phone number associated with a student.\n",
    "URL_PERSONAL - A URL that might be used to identify a student.\n",
    "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
    "\n",
    "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
    "\n",
    "TEXT:\n",
    "{prompt}\n",
    "OUTPUT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sequence_indices(list_words, sequence_to_find):\n",
    "    sequence_length = len(sequence_to_find)\n",
    "    indices = [i for i in range(len(list_words) - sequence_length + 1) if list_words[i:i+sequence_length] == sequence_to_find]\n",
    "    return indices\n",
    "\n",
    "def llama_to_tokens(output):\n",
    "    nlp = English()\n",
    "\n",
    "    english_tokenizer = nlp.tokenizer\n",
    "\n",
    "    tokens = []\n",
    "    labels = []\n",
    "\n",
    "    answers = re.split(r'\\n',output)\n",
    "    for i in range(len(answers)):\n",
    "        tokens.append(re.split(r'\\(|\\)', answers[i])[:-1])\n",
    "        labels.append(tokens[-1][-1])\n",
    "        tokens[-1] = tokens[-1][:-1]\n",
    "\n",
    "    # print('Tokens', tokens)\n",
    "    # print('Labels', labels)\n",
    "    for i in range(len(tokens)):\n",
    "        # print(tokens[i][0])\n",
    "        tokenized = english_tokenizer(tokens[i][0])\n",
    "        tokens[i] = [i.text for i in tokenized]\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "def categorizer(full_token_list, llm_tokens, labels):\n",
    "    indices = []\n",
    "    for i in range(len(llm_tokens)):\n",
    "        indices.append(find_sequence_indices(full_token_list, llm_tokens[i]))\n",
    "    # print(\"Indices\", indices)\n",
    "    result = ['O'] * len(full_token_list) # This will be a list of length full_tokens_list\n",
    "\n",
    "    for k in range(len(llm_tokens)):\n",
    "        for i in range(len(indices[k])):\n",
    "            result[indices[k][i]] = 'B-'+labels[k]\n",
    "            if len(llm_tokens[k])>1:\n",
    "                for l in range(len(llm_tokens[k])-1):\n",
    "                    result[indices[k][i]+l+1] = 'I-' + labels[k]\n",
    "\n",
    "    return result[:len(full_token_list)]\n",
    "\n",
    "def assign_labels(full_text, output_text):\n",
    "    # print('full_text:',full_text)\n",
    "    tokenized = english_tokenizer(full_text)\n",
    "    full_text_tokens = [i.text for i in tokenized]\n",
    "    # print(\"Full Text Tokens:\", full_text_tokens)\n",
    "    # print('LLM Output:', output_text)\n",
    "\n",
    "    text_tokens, labels = llama_to_tokens(output_text)\n",
    "    # print('Text tokens:',text_tokens,'Labels:',labels)\n",
    "\n",
    "    labeled_output = categorizer(full_text_tokens,text_tokens, labels)\n",
    "    # print('Final Output:', labeled_output)\n",
    "    return labeled_output\n",
    "\n",
    "def curate_labels(labeled_tokens):\n",
    "    label_pattern = pii_labels_pattern + \"|O\"\n",
    "\n",
    "    for i in range(len(labeled_tokens)):\n",
    "        if(not re.search(label_pattern, labeled_tokens[i])):\n",
    "            labeled_tokens[i] = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  6807\n",
      "Test Data:  10\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"pii-detection-data/train.json\"\n",
    "test_data_path = \"pii-detection-data/test.json\"\n",
    "\n",
    "# Loading Dataset\n",
    "with open(train_data_path) as file:\n",
    "    train_data_json = json.load(file)\n",
    "    print(\"Training Data: \", len(train_data_json))\n",
    "\n",
    "with open(test_data_path ) as file:\n",
    "    test_data_json = json.load(file)\n",
    "    print(\"Test Data: \", len(test_data_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size:  13\n"
     ]
    }
   ],
   "source": [
    "# Limiting the data for testing\n",
    "train_data_size = int(len(train_data_json) * 0.002)\n",
    "print(\"Train Data Size: \", train_data_size)\n",
    "\n",
    "train_data = train_data_json[:train_data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Sample: 0\n",
      "\tProcessing Batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/self_rag/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing Batch: 1\n",
      "\tProcessing Batch: 2\n",
      "\tProcessing Batch: 3\n",
      "\tProcessing Batch: 4\n",
      "Processing Sample: 1\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "Processing Sample: 2\n",
      "\tProcessing Batch: 0\n",
      "Processing Sample: 3\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "Processing Sample: 4\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "Processing Sample: 5\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "\tProcessing Batch: 2\n",
      "\tProcessing Batch: 3\n",
      "\tProcessing Batch: 4\n",
      "Processing Sample: 6\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "Processing Sample: 7\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "\tProcessing Batch: 2\n",
      "Processing Sample: 8\n",
      "\tProcessing Batch: 0\n",
      "\tProcessing Batch: 1\n",
      "\tProcessing Batch: 2\n",
      "Misclassification: 0.03210433910208176\n",
      "Accuracy: 0.9678956608979182\n"
     ]
    }
   ],
   "source": [
    "train_text_input_ids = []\n",
    "train_labels_input_ids = []\n",
    "max_length = 400\n",
    "total_classifications = 0\n",
    "num_misclassified = 0\n",
    "try:\n",
    "    for i, data in enumerate(train_data):\n",
    "        print(\"Processing Sample:\", i)\n",
    "        # Loop through data in batches of 400 tokens\n",
    "        for j in range(0, len(data[\"tokens\"]), max_length):\n",
    "            print(\"\\tProcessing Batch:\", int(j / max_length))\n",
    "            batch_size = min(j + max_length, len(data[\"tokens\"]))\n",
    "            input_text = \" \".join(data[\"tokens\"][j: batch_size])\n",
    "            output_labels = data[\"labels\"][j: batch_size]\n",
    "\n",
    "            prompt = format_prompt(input_text)\n",
    "            \n",
    "            sequences = pipeline(\n",
    "                format_prompt(input_text),\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                # max_length=1500,\n",
    "                temperature=0.0001,\n",
    "            )\n",
    "\n",
    "            # Process output text\n",
    "            outputs = re.split(r',?\\n', sequences[0]['generated_text'].replace(prompt, \"\"))\n",
    "            outputs = [output.strip() for output in outputs if re.search(f\"\\(({pii_labels_pattern})\\)\", output)]\n",
    "            if(not outputs):\n",
    "                total_classifications += len(output_labels)\n",
    "                num_misclassified += len(output_labels) - output_labels.count('O')\n",
    "                continue\n",
    "            output_text = '\\n'.join(outputs)\n",
    "\n",
    "            # Assigning Labels\n",
    "            labeled_output = assign_labels(input_text, output_text)\n",
    "            curate_labels(labeled_output)\n",
    "\n",
    "            # print(\"Final Output:\", output_labels)\n",
    "\n",
    "            assert len(output_labels) == len(labeled_output)\n",
    "\n",
    "            # Comparing output with expected labels\n",
    "            total_classifications += len(labeled_output)\n",
    "            for i in range(len(labeled_output)):\n",
    "                if(labeled_output[i] != output_labels[i]):\n",
    "                    num_misclassified += 1\n",
    "\n",
    "        print()\n",
    "        print(\"Misclassification:\", num_misclassified / total_classifications)\n",
    "        print(\"Accuracy:\", (total_classifications - num_misclassified) / total_classifications)\n",
    "        print()\n",
    "        \n",
    "except Exception as error:\n",
    "    print(\"\\nError Occured for the following input:\")\n",
    "    print(\"INPUT\", input_text)\n",
    "    print(\"EXPECTED OUTPUT\", output_labels)\n",
    "    print(\"OUTPUT TEXT\", sequences[0]['generated_text'].replace(prompt, \"\"))\n",
    "    print(\"PROCESSED OUTPUT\", outputs)\n",
    "    print(\"LABELED OUTPUT\", labeled_output)\n",
    "\n",
    "    print(\"\\nError:\\n\", error)\n",
    "\n",
    "                                   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Testing (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"the   urgent  and  compelling  need  for  the  trail  in  a  succinct  and  tangible  way .    Application    Once  we  drafted  the  vision  document ,  we  worked  with  the  charity  to  identify  a  list  of  people  whose   opinion  would  be  important  to  the  success  ( or  failure )  of  the  fundraising  campaign .  The  list  included   past  and  potential  donors ,  key  influencers  in  the  community  such  as  large  landowners  and  business   owners ,  affluent  summer-only  residents ,  and  elected  officials .  We  requested  one-hour  meetings  with   all  of  the  people  on  the  list .  If  people  did  not  want  to  meet  with  us  in  person ,  which  was  often  the    case  with  the  part-time  residents ,  we  offered  to  conduct  the  meetings  by  phone .  When  someone   agreed  to  meet  with  us ,  we  emailed  them  the  vision  document  so  they  could  read  it  in  advance  and   prepare  their  questions .  This  created  a  good  environment  for  an  informed  and  candid  dialogue .    While  the  scheduling  of  the  interviews  was  in  progress ,  we  designed  a  questionnaire  to  guide  our   discussions .  Consistently  using  the  questionnaire  ensured  that  we  covered  the  same  questions  with  all   the  interviewees .  The  goal  was  to  speak  with  20  -  25  key  influencers  in\"\n",
    "prompt = format_prompt(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision document (DOCUMENT),\n",
      "past and potential donors (PHONE_NUM),\n",
      "key influencers in the community (PHONE_NUM),\n",
      "affluent summer-only residents (PHONE_NUM),\n",
      "elected officials (PHONE_NUM),\n",
      "one-hour meetings (MEETING),\n",
      "part-time residents (PHONE_NUM),\n",
      "email (EMAIL),\n",
      "informed and candid dialogue (GOAL)\n",
      "\n",
      "You are searching for these different types of words:\n",
      "\n",
      "NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.\n",
      "EMAIL - A student's email address.\n",
      "USERNAME - A student's username on any platform.\n",
      "ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.\n",
      "PHONE_NUM - A phone number associated with a student.\n",
      "URL_PERSONAL - A URL that might be used to identify a student.\n",
      "STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.\n",
      "\n",
      "You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.\n",
      "\n",
      "TEXT:\n",
      "The student council president is John Smith. His email is jsmith@student.com and his phone number is 555-555-5555.\n",
      "OUTPUT:\n",
      "John Smith (NAME_STUDENT),\n",
      "jsmith@student.com (EMAIL),\n",
      "555-555-5555 (PHONE_NUM)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # max_length=1500,\n",
    "    temperature=0.0001,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\".replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vision document (DOCUMENT)',\n",
       " 'past and potential donors (PHONE_NUM)',\n",
       " 'key influencers in the community (PHONE_NUM)',\n",
       " 'affluent summer-only residents (PHONE_NUM)',\n",
       " 'elected officials (PHONE_NUM)',\n",
       " 'one-hour meetings (MEETING)',\n",
       " 'part-time residents (PHONE_NUM)',\n",
       " 'email (EMAIL)',\n",
       " 'informed and candid dialogue (GOAL)',\n",
       " '',\n",
       " 'You are searching for these different types of words:',\n",
       " '',\n",
       " 'NAME_STUDENT - The full or partial name of a student that is not necessarily the author of the essay. This excludes instructors, authors, and other person names.',\n",
       " \"EMAIL - A student's email address.\",\n",
       " \"USERNAME - A student's username on any platform.\",\n",
       " 'ID_NUM - A number or sequence of characters that could be used to identify a student, such as a student ID or a social security number.',\n",
       " 'PHONE_NUM - A phone number associated with a student.',\n",
       " 'URL_PERSONAL - A URL that might be used to identify a student.',\n",
       " 'STREET_ADDRESS - A full or partial street address that is associated with the student, such as their home address.',\n",
       " '',\n",
       " 'You will be given a TEXT, and your OUTPUT will be a list of each instance of words belonging to the previous category and which category they are.',\n",
       " '',\n",
       " 'TEXT:',\n",
       " 'The student council president is John Smith. His email is jsmith@student.com and his phone number is 555-555-5555.',\n",
       " 'OUTPUT:',\n",
       " 'John Smith (NAME_STUDENT)',\n",
       " 'jsmith@student.com (EMAIL)',\n",
       " '555-555-5555 (PHONE_NUM)']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print(len(prompt))\n",
    "outputs = re.split(r',?\\n', seq['generated_text'].replace(prompt, \"\"))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['past and potential donors (PHONE_NUM)',\n",
       " 'key influencers in the community (PHONE_NUM)',\n",
       " 'affluent summer-only residents (PHONE_NUM)',\n",
       " 'elected officials (PHONE_NUM)',\n",
       " 'part-time residents (PHONE_NUM)',\n",
       " 'email (EMAIL)',\n",
       " 'John Smith (NAME_STUDENT)',\n",
       " 'jsmith@student.com (EMAIL)',\n",
       " '555-555-5555 (PHONE_NUM)']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pii_labels_pattern = '|'.join(pii_labels)\n",
    "outputs = [output.strip() for output in outputs if re.search(f\"\\(({pii_labels_pattern})\\)\", output)]\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'past and potential donors (PHONE_NUM)\\nkey influencers in the community (PHONE_NUM)\\naffluent summer-only residents (PHONE_NUM)\\nelected officials (PHONE_NUM)\\npart-time residents (PHONE_NUM)\\nemail (EMAIL)\\nJohn Smith (NAME_STUDENT)\\njsmith@student.com (EMAIL)\\n555-555-5555 (PHONE_NUM)'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = '\\n'.join(outputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "english_tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text: the   urgent  and  compelling  need  for  the  trail  in  a  succinct  and  tangible  way .    Application    Once  we  drafted  the  vision  document ,  we  worked  with  the  charity  to  identify  a  list  of  people  whose   opinion  would  be  important  to  the  success  ( or  failure )  of  the  fundraising  campaign .  The  list  included   past  and  potential  donors ,  key  influencers  in  the  community  such  as  large  landowners  and  business   owners ,  affluent  summer-only  residents ,  and  elected  officials .  We  requested  one-hour  meetings  with   all  of  the  people  on  the  list .  If  people  did  not  want  to  meet  with  us  in  person ,  which  was  often  the    case  with  the  part-time  residents ,  we  offered  to  conduct  the  meetings  by  phone .  When  someone   agreed  to  meet  with  us ,  we  emailed  them  the  vision  document  so  they  could  read  it  in  advance  and   prepare  their  questions .  This  created  a  good  environment  for  an  informed  and  candid  dialogue .    While  the  scheduling  of  the  interviews  was  in  progress ,  we  designed  a  questionnaire  to  guide  our   discussions .  Consistently  using  the  questionnaire  ensured  that  we  covered  the  same  questions  with  all   the  interviewees .  The  goal  was  to  speak  with  20  -  25  key  influencers  in\n",
      "Full Text Tokens: ['the', '  ', 'urgent', ' ', 'and', ' ', 'compelling', ' ', 'need', ' ', 'for', ' ', 'the', ' ', 'trail', ' ', 'in', ' ', 'a', ' ', 'succinct', ' ', 'and', ' ', 'tangible', ' ', 'way', '.', '   ', 'Application', '   ', 'Once', ' ', 'we', ' ', 'drafted', ' ', 'the', ' ', 'vision', ' ', 'document', ',', ' ', 'we', ' ', 'worked', ' ', 'with', ' ', 'the', ' ', 'charity', ' ', 'to', ' ', 'identify', ' ', 'a', ' ', 'list', ' ', 'of', ' ', 'people', ' ', 'whose', '  ', 'opinion', ' ', 'would', ' ', 'be', ' ', 'important', ' ', 'to', ' ', 'the', ' ', 'success', ' ', '(', 'or', ' ', 'failure', ')', ' ', 'of', ' ', 'the', ' ', 'fundraising', ' ', 'campaign', '.', ' ', 'The', ' ', 'list', ' ', 'included', '  ', 'past', ' ', 'and', ' ', 'potential', ' ', 'donors', ',', ' ', 'key', ' ', 'influencers', ' ', 'in', ' ', 'the', ' ', 'community', ' ', 'such', ' ', 'as', ' ', 'large', ' ', 'landowners', ' ', 'and', ' ', 'business', '  ', 'owners', ',', ' ', 'affluent', ' ', 'summer', '-', 'only', ' ', 'residents', ',', ' ', 'and', ' ', 'elected', ' ', 'officials', '.', ' ', 'We', ' ', 'requested', ' ', 'one', '-', 'hour', ' ', 'meetings', ' ', 'with', '  ', 'all', ' ', 'of', ' ', 'the', ' ', 'people', ' ', 'on', ' ', 'the', ' ', 'list', '.', ' ', 'If', ' ', 'people', ' ', 'did', ' ', 'not', ' ', 'want', ' ', 'to', ' ', 'meet', ' ', 'with', ' ', 'us', ' ', 'in', ' ', 'person', ',', ' ', 'which', ' ', 'was', ' ', 'often', ' ', 'the', '   ', 'case', ' ', 'with', ' ', 'the', ' ', 'part', '-', 'time', ' ', 'residents', ',', ' ', 'we', ' ', 'offered', ' ', 'to', ' ', 'conduct', ' ', 'the', ' ', 'meetings', ' ', 'by', ' ', 'phone', '.', ' ', 'When', ' ', 'someone', '  ', 'agreed', ' ', 'to', ' ', 'meet', ' ', 'with', ' ', 'us', ',', ' ', 'we', ' ', 'emailed', ' ', 'them', ' ', 'the', ' ', 'vision', ' ', 'document', ' ', 'so', ' ', 'they', ' ', 'could', ' ', 'read', ' ', 'it', ' ', 'in', ' ', 'advance', ' ', 'and', '  ', 'prepare', ' ', 'their', ' ', 'questions', '.', ' ', 'This', ' ', 'created', ' ', 'a', ' ', 'good', ' ', 'environment', ' ', 'for', ' ', 'an', ' ', 'informed', ' ', 'and', ' ', 'candid', ' ', 'dialogue', '.', '   ', 'While', ' ', 'the', ' ', 'scheduling', ' ', 'of', ' ', 'the', ' ', 'interviews', ' ', 'was', ' ', 'in', ' ', 'progress', ',', ' ', 'we', ' ', 'designed', ' ', 'a', ' ', 'questionnaire', ' ', 'to', ' ', 'guide', ' ', 'our', '  ', 'discussions', '.', ' ', 'Consistently', ' ', 'using', ' ', 'the', ' ', 'questionnaire', ' ', 'ensured', ' ', 'that', ' ', 'we', ' ', 'covered', ' ', 'the', ' ', 'same', ' ', 'questions', ' ', 'with', ' ', 'all', '  ', 'the', ' ', 'interviewees', '.', ' ', 'The', ' ', 'goal', ' ', 'was', ' ', 'to', ' ', 'speak', ' ', 'with', ' ', '20', ' ', '-', ' ', '25', ' ', 'key', ' ', 'influencers', ' ', 'in']\n",
      "LLM Output: past and potential donors (PHONE_NUM)\n",
      "key influencers in the community (PHONE_NUM)\n",
      "affluent summer-only residents (PHONE_NUM)\n",
      "elected officials (PHONE_NUM)\n",
      "part-time residents (PHONE_NUM)\n",
      "email (EMAIL)\n",
      "John Smith (NAME_STUDENT)\n",
      "jsmith@student.com (EMAIL)\n",
      "555-555-5555 (PHONE_NUM)\n",
      "Text tokens: [['past', 'and', 'potential', 'donors'], ['key', 'influencers', 'in', 'the', 'community'], ['affluent', 'summer', '-', 'only', 'residents'], ['elected', 'officials'], ['part', '-', 'time', 'residents'], ['email'], ['John', 'Smith'], ['jsmith@student.com'], ['555', '-', '555', '-', '5555']] Labels: ['PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'PHONE_NUM', 'EMAIL', 'NAME_STUDENT', 'EMAIL', 'PHONE_NUM']\n",
      "Final Output: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Testing with actual input text and output\n",
    "full_text = input_text\n",
    "print('full_text:',full_text)\n",
    "tokenized = english_tokenizer(full_text)\n",
    "full_text_tokens = [i.text for i in tokenized]\n",
    "print(\"Full Text Tokens:\", full_text_tokens)\n",
    "print('LLM Output:', output)\n",
    "\n",
    "text_tokens, labels = llama_to_tokens(output)\n",
    "print('Text tokens:',text_tokens,'Labels:',labels)\n",
    "\n",
    "print('Final Output:',categorizer(full_text_tokens,text_tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text: The strange thing said, \"Beep\". I called into the fog, \"What did you say?\" Out as a response was, \"Beep!\"\n",
      "Full Text Tokens: ['The', 'strange', 'thing', 'said', ',', '\"', 'Beep', '\"', '.', 'I', 'called', 'into', 'the', 'fog', ',', '\"', 'What', 'did', 'you', 'say', '?', '\"', 'Out', 'as', 'a', 'response', 'was', ',', '\"', 'Beep', '!', '\"']\n",
      "LLM Output: Beep (HIVER)\n",
      "What did you say? (GREEN_LANDER)\n",
      "Text tokens: [['Beep'], ['What', 'did', 'you', 'say', '?']] Labels: ['HIVER', 'GREEN_LANDER']\n",
      "Final Output: ['O', 'O', 'O', 'O', 'O', 'O', 'B-HIVER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-HIVER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\"\"The strange thing said, \\\"Beep\\\". I called into the fog, \\\"What did you say?\\\" Out as a response was, \\\"Beep!\\\"\"\"\"\n",
    "print('full_text:',full_text)\n",
    "nlp = English()\n",
    "english_tokenizer = nlp.tokenizer\n",
    "tokenized = english_tokenizer(full_text)\n",
    "full_text_tokens = [i.text for i in tokenized]\n",
    "print(\"Full Text Tokens:\", full_text_tokens)\n",
    "\n",
    "# print(full_text)\n",
    "text = \"\"\"Beep (HIVER)\n",
    "What did you say? (GREEN_LANDER)\"\"\"\n",
    "print('LLM Output:',text)\n",
    "\n",
    "text_tokens, labels = llama_to_tokens(text)\n",
    "print('Text tokens:',text_tokens,'Labels:',labels)\n",
    "\n",
    "\n",
    "print('Final Output:',categorizer(full_text_tokens, text_tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices [[6, 29], [16]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-HIVER', 'I', '-', 'H', 'I', 'V', 'E', 'R', 'O', 'O', 'B-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'I-GREEN_LANDER', 'V', 'E', 'R', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'EMAIL']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'EMAIL']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tokens = categorizer(full_text_tokens,text_tokens, labels)\n",
    "labeled_tokens.append('EMAIL')\n",
    "print(labeled_tokens)\n",
    "curate_labels(labeled_tokens)\n",
    "labeled_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices [[6, 29], [16]]\n",
      "Trigered: B-HIVER\n",
      "Trigered: I\n",
      "Trigered: -\n",
      "Trigered: H\n",
      "Trigered: I\n",
      "Trigered: V\n",
      "Trigered: E\n",
      "Trigered: R\n",
      "Trigered: B-GREEN_LANDER\n",
      "Trigered: I-GREEN_LANDER\n",
      "Trigered: I-GREEN_LANDER\n",
      "Trigered: I-GREEN_LANDER\n",
      "Trigered: I-GREEN_LANDER\n",
      "Trigered: V\n",
      "Trigered: E\n",
      "Trigered: R\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tokens = categorizer(full_text_tokens,text_tokens, labels)\n",
    "for i in range(len(labeled_tokens)):\n",
    "    if(not re.search(pii_labels_pattern + \"|O\", labeled_tokens[i])):\n",
    "       print(\"Trigered:\", labeled_tokens[i])\n",
    "       labeled_tokens[i] = 'O'\n",
    "labeled_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
